{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q6uQ-vbVwbMX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import absl.logging\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as tf_text  # A dependency of the preprocessing model\n",
        "from official import nlp\n",
        "import official.nlp.optimization\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import time\n",
        "from fastprogress import master_bar, progress_bar\n",
        "import math\n",
        "import official.nlp.bert.tokenization\n",
        "from official.nlp import bert\n",
        "\n",
        "def load_file(file_ds):\n",
        "    with open(f'dataset/{file_ds}.tsv','r') as file:\n",
        "        test_file = file.readlines()\n",
        "\n",
        "    function_sentence = []\n",
        "    label_test = []\n",
        "    temp_ds = []\n",
        "    temp_label = []\n",
        "    for line in test_file:\n",
        "        if line == '\\n':  \n",
        "            if len(temp_label) > 128:\n",
        "                temp_label = temp_label[:128]\n",
        "            if len(temp_label) < 128:\n",
        "                temp_label.extend([0]*(128-len(temp_label)))\n",
        "            if ' '.join(temp_ds) not in function_sentence:\n",
        "                if temp_label and temp_ds:\n",
        "                    function_sentence.append(' '.join(temp_ds))\n",
        "                    label_test.append(temp_label)\n",
        "            temp_ds = []\n",
        "            temp_label = []\n",
        "        else:\n",
        "            temp_ds.append(line.split('\\t')[0].replace('\\n',''))\n",
        "            cadena = line.split('\\t')[1].replace('\\n','')\n",
        "            temp_label.append(number_category(cadena))\n",
        "\n",
        "    return function_sentence,label_test\n",
        "\n",
        "def number_category(name_category):\n",
        "    category_dict = {\n",
        "      \"O\":1,\n",
        "      \"B-Gversion\":2,\n",
        "      \"B-Technique\":4,\n",
        "      \"B-Med\":6,\n",
        "      \"B-Gtype\":8,\n",
        "      \"B-Orgn\":10,\n",
        "      \"B-Strain\":12,\n",
        "      \"B-Air\":13,\n",
        "      \"B-Substrain\":12,\n",
        "      \"B-pH\":26,\n",
        "      \"B-Supp\":15,\n",
        "      \"B-Vess\":12,\n",
        "      \"B-Agit\":17,\n",
        "      \"B-Anti\":19,\n",
        "      \"B-OD\":21,\n",
        "      \"B-Phase\":23,\n",
        "      \"B-Temp\":25,\n",
        "      \n",
        "      \"I-Gversion\":3,\n",
        "      \"I-Technique\":5,\n",
        "      \"I-Med\":7,\n",
        "      \"I-Gtype\":9,\n",
        "      \"I-Orgn\":11,\n",
        "      \"I-Strain\":12,\n",
        "      \"I-Air\":14,\n",
        "      \"I-Substrain\":12,\n",
        "      \"I-pH\":26,\n",
        "      \"I-Supp\":16,\n",
        "      \"I-Vess\":12,\n",
        "      \"I-Agit\":18,\n",
        "      \"I-Anti\":20,\n",
        "      \"I-OD\":22,\n",
        "      \"I-Phase\":24,\n",
        "      \"I-Temp\":25,\n",
        "      }\n",
        "    return category_dict[name_category]\n",
        "\n",
        "\n",
        "def build_classifier_model(num_classes):\n",
        "    '''\n",
        "    Fine tunes BERT.\n",
        "    Input: Dataset made by a bert preprocessor e.i \"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\"\n",
        "        1. input_word_ids, input_mask, input_type_ids\n",
        "        2. Labels for each word. \n",
        "    Output: Trained model.\n",
        "    '''\n",
        "\n",
        "    inputs = dict(\n",
        "        input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
        "        input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
        "        input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
        "    )\n",
        "  \n",
        "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='encoder')\n",
        "    net = encoder(inputs)['sequence_output']\n",
        "    net = tf.keras.layers.Dropout(rate=0.1)(net)\n",
        "    net = tf.keras.layers.Dense(num_classes, activation='softmax', name='classifier')(net)\n",
        "    return tf.keras.Model(inputs, net, name='prediction')\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    '''\n",
        "    Controls what is happening during training.\n",
        "    '''\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = bert_classifier(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, bert_classifier.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, bert_classifier.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    '''\n",
        "    Computes the validation accuracy for each epoch during training.\n",
        "    '''\n",
        "    val_logits = bert_classifier(x, training=False)\n",
        "    val_acc_metric.update_state(y, val_logits)\n",
        "    \n",
        "\n",
        "def test_result(file_ds):\n",
        "    with open(f'dataset/{file_ds}.tsv','r') as file:\n",
        "        test_file = file.readlines()\n",
        "\n",
        "    function_sentence = []\n",
        "    label_test = []\n",
        "    temp_ds = []\n",
        "    temp_label = []\n",
        "    for line in test_file:\n",
        "        if line == '\\n':  \n",
        "            if len(temp_label) > 128:\n",
        "                temp_label = temp_label[:128]\n",
        "            if ' '.join(temp_ds) not in function_sentence:\n",
        "                if temp_label and temp_ds:\n",
        "                    function_sentence.append(' '.join(temp_ds))\n",
        "                    label_test.append(temp_label)\n",
        "            temp_ds = []\n",
        "            temp_label = []\n",
        "        else:\n",
        "            temp_ds.append(line.split('\\t')[0].replace('\\n',''))\n",
        "            temp_label.append(line.split('\\t')[1].replace('\\n',''))\n",
        "    return function_sentence,label_test\n",
        "\n",
        "\n",
        "preprocessor = hub.load(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "tfhub_handle_encoder = \"https://tfhub.dev/google/experts/bert/pubmed/2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vAwr2t4-wbMd"
      },
      "outputs": [],
      "source": [
        "# Set up model, epochs and steps\n",
        "epochs = 4\n",
        "batch_size = 8\n",
        "\n",
        "\n",
        "# test_text, test_tags, unique_test_labels = load_file('test')\n",
        "# test_sentences = preprocessor(test_text)\n",
        "train_text, tags = load_file('train')\n",
        "train_sentences = preprocessor(train_text)\n",
        "tokens = preprocessor.tokenize(train_text)\n",
        "\n",
        "\n",
        "# Since BERT tokenizer splits single words (interestingly -> interest, ##ing, ##ly), we need to add labels for each additional token\n",
        "              #data                 #labels\n",
        "# Before:   interest ##ing #ly      -> O\n",
        "# After:    interest ##ing #ly      -> O, 0, 0\n",
        "train_labels = []\n",
        "for num_s, sentence in enumerate(tokens):\n",
        "    temp = []\n",
        "    for num_w, fword in enumerate(sentence):\n",
        "        word = list(fword.numpy())\n",
        "        if num_w <= len(tags[num_s]):\n",
        "            old_tag = tags[num_s][num_w]\n",
        "            if len(word) >= 1:\n",
        "                temp.append(old_tag)\n",
        "            for times in range(len(word)-1):\n",
        "                temp.append(old_tag)\n",
        "    l = len(temp)\n",
        "    temp.extend([0]*(128-l))\n",
        "    temp.insert(0,0)\n",
        "    train_labels.append(tf.constant(temp[0:128]))\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences,train_labels)) # Builds dataset in a format that can be fed to the model.\n",
        "train_ds = train_dataset.shuffle(len(tags),reshuffle_each_iteration=False)\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "\n",
        "\n",
        "# Same as above but fot the test dataset.\n",
        "test_text, test_tags = load_file('test')\n",
        "test_sentences = preprocessor(test_text)\n",
        "tokens = preprocessor.tokenize(test_text)\n",
        "test_labels = []\n",
        "for num_s, sentence in enumerate(tokens):\n",
        "    temp = []\n",
        "    for num_w, fword in enumerate(sentence):\n",
        "        word = list(fword.numpy())\n",
        "        if num_w <= len(test_tags[num_s]):\n",
        "            old_tag = test_tags[num_s][num_w]\n",
        "            if len(word) >= 1:\n",
        "                temp.append(old_tag)\n",
        "            for times in range(len(word)-1):\n",
        "                temp.append(old_tag)\n",
        "    l = len(temp)\n",
        "    temp.extend([0]*(128-l))\n",
        "    temp.insert(0,0)\n",
        "    test_labels.append(tf.constant(temp[0:128]))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((test_sentences,test_labels))\n",
        "val_ds = val_dataset.batch(batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# creates an optimizer with learning rate schedule\n",
        "train_data_size = len(tags)\n",
        "steps_per_epoch = int(train_data_size / batch_size)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
        "optimizer = nlp.optimization.create_optimizer(5e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
        "\n",
        "\n",
        "# Metrics and loss that can be used by a multiclass classificator\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "bert_classifier = build_classifier_model(27)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert_classifier.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "R2UX3WpIwbMd",
        "outputId": "c14c5950-004e-45d1-afa9-032c40fe9d71"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 6.2161455154418945\n",
            "Training accuracy over epoch 0: 0.7471411824226379\n",
            "Time taken: 269.8068301677704\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 0: 0.19663934409618378\n",
            "Training accuracy over epoch 1: 0.9581032991409302\n",
            "Time taken: 270.4076099395752\n",
            "\n",
            "Start of epoch 3\n",
            "Training loss (for one batch) at step 0: 0.12945497035980225\n",
            "Training accuracy over epoch 2: 0.9689717888832092\n",
            "Time taken: 279.6406960487366\n",
            "\n",
            "Start of epoch 4\n",
            "Training loss (for one batch) at step 0: 0.08624684810638428\n",
            "Training accuracy over epoch 3: 0.9768335819244385\n",
            "Time taken: 240.59756016731262\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Custom training loop. https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n",
        "epoch_bar = master_bar(range(epochs))\n",
        "pb_max_len = math.ceil(float(len(train_text))/float(batch_size))\n",
        "\n",
        "for epoch in epoch_bar:\n",
        "    print(\"\\nStart of epoch %d\" % (epoch + 1,))\n",
        "    start_time = time.time()\n",
        "    for step, (x_batch_train, y_batch_train) in progress_bar(enumerate(train_ds),total=pb_max_len, parent=epoch_bar):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "        \n",
        "        if step % 50 == 0:\n",
        "            print(f\"Training loss (for one batch) at step {step}: {loss_value}\")\n",
        "\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(f\"Training accuracy over epoch {epoch}: {float(train_acc)}\")\n",
        "    train_acc_metric.reset_states()\n",
        "    \n",
        "    # Run a validationghjl loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_ds:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    print(f\"Validation acc: {float(val_acc)}\")\n",
        "    print(f\"Time taken: {time.time() - start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ut7BjdekwbMe",
        "outputId": "5b4af043-6223-4a7b-d07c-223dd8d2d55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"prediction\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " encoder (KerasLayer)           {'sequence_output':  109482241   ['input_2[0][0]',                \n",
            "                                 (None, None, 768),               'input_3[0][0]',                \n",
            "                                 'encoder_outputs':               'input_1[0][0]']                \n",
            "                                 [(None, None, 768)                                               \n",
            "                                , (None, None, 768)                                               \n",
            "                                , (None, None, 768)                                               \n",
            "                                , (None, None, 768)                                               \n",
            "                                , (None, None, 768)                                               \n",
            "                                , (None, None, 768)                                               \n",
            "                                , (None, None, 768)                                               \n",
            "                                , (None, None, 768)                                               \n",
            "                                , (None, None, 768)                                               \n",
            "                                , (None, None, 768)                                               \n",
            "                                , (None, None, 768)                                               \n",
            "                                , (None, None, 768)                                               \n",
            "                                ],                                                                \n",
            "                                 'pooled_output': (                                               \n",
            "                                None, 768),                                                       \n",
            "                                 'default': (None,                                                \n",
            "                                768)}                                                             \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, None, 768)    0           ['encoder[0][14]']               \n",
            "                                                                                                  \n",
            " classifier (Dense)             (None, None, 27)     20763       ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,503,004\n",
            "Trainable params: 109,503,003\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 360). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: TrainedModel/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: TrainedModel/assets\n"
          ]
        }
      ],
      "source": [
        "# Prints a model summary and saves the model.\n",
        "bert_classifier.summary()\n",
        "bert_classifier.save('TrainedModel/', include_optimizer=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hgTsuchkwbMe",
        "outputId": "f4428393-beef-43fc-83c8-db6944ad4167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "test_text, exp_test = test_result('test')\n",
        "test_x = preprocessor(test_text)\n",
        "bert_classifier = tf.keras.models.load_model('TrainedModel')  # Loads the NER model trained with the Ner_Training.ipynb notebook\n",
        "prediction = bert_classifier.predict(test_x) # Predicts the label for each token of the preprocessed sentences\n",
        "tokens = preprocessor.tokenize(test_text)\n",
        "exp_labels = []\n",
        "for num_s, sentence in enumerate(tokens):\n",
        "    temp = []\n",
        "    for num_w, fword in enumerate(sentence):\n",
        "        word = list(fword.numpy())\n",
        "        if num_w < len(exp_test[num_s]):\n",
        "            old_tag = exp_test[num_s][num_w]\n",
        "            if len(word) >= 1:\n",
        "                temp.append(old_tag)\n",
        "            for times in range(len(word)-1):\n",
        "                temp.append(old_tag)\n",
        "    l = len(temp)\n",
        "    temp.extend([0]*(128-l))\n",
        "    temp.insert(0,0)\n",
        "    exp_labels.append(temp)\n",
        "\n",
        "# The \"prediction\" variable has a score for each of the possible categories for a token'\n",
        "#    ([CLS],[SEP],[PAD])        O          B-GENE      I-GENE    -> Index (0:3)\n",
        "#           0.01              0.90         0.05      0.04        -> Predicted score for a single token\n",
        "# THE SIZE OF PREDICTION IS:     (4) x (Num of tokens in a sentence) x (Total num of sentences)\n",
        "# This part finds in which index of \"prediction[sentence x][token x]\" has the bigest number, then saves the asociated label for that index\n",
        "sentence_tags = []\n",
        "raw_sentences = []\n",
        "for i, sentence in enumerate(prediction):\n",
        "    temp_rel = []\n",
        "    for n_wor, pred_word in enumerate(sentence):\n",
        "        val = list(pred_word)\n",
        "        if val[1] == max(val):\n",
        "            temp_rel.append('O')\n",
        "        elif val[2] == max(val) or val[3] == max(val):\n",
        "            temp_rel.append('Gversion')\n",
        "        elif val[4] == max(val) or val[5] == max(val):\n",
        "            temp_rel.append('Technique')\n",
        "        elif val[6] == max(val) or val[7] == max(val):\n",
        "            temp_rel.append('Med')\n",
        "        elif val[8] == max(val) or val[9] == max(val):\n",
        "            temp_rel.append('Gtype')\n",
        "        elif val[10] == max(val) or val[11] == max(val):\n",
        "            temp_rel.append('Orgn')\n",
        "        elif val[12] == max(val):\n",
        "            temp_rel.append('Strain')\n",
        "        elif val[13] == max(val) or val[14] == max(val):\n",
        "            temp_rel.append('Air')\n",
        "        elif val[12] == max(val):\n",
        "            temp_rel.append('Substrain')\n",
        "        elif val[26] == max(val):\n",
        "            temp_rel.append('pH')\n",
        "        elif val[15] == max(val) or val[16] == max(val):\n",
        "            temp_rel.append('Supp')\n",
        "        elif val[12] == max(val):\n",
        "            temp_rel.append('Vess')\n",
        "        elif val[17] == max(val) or val[18] == max(val):\n",
        "            temp_rel.append('Agit')\n",
        "        elif val[19] == max(val) or val[20] == max(val):\n",
        "            temp_rel.append('Anti')\n",
        "        elif val[21] == max(val) or val[22] == max(val):\n",
        "            temp_rel.append('OD')\n",
        "        elif val[23] == max(val) or val[24] == max(val):\n",
        "            temp_rel.append('Phase')\n",
        "        elif val[25] == max(val):\n",
        "            temp_rel.append('Temp')\n",
        "    #raw_sentences.append(pre_txt[i])\n",
        "    sentence_tags.append(temp_rel)\n",
        "    \n",
        "tokenizer = bert.tokenization.FullTokenizer('vocab.txt', do_lower_case=True)\n",
        "pre_txt = []\n",
        "for indx in range(len(test_text)):\n",
        "    pre_txt.append(tokenizer.tokenize(test_text[indx]))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "354 39 317 356\n"
          ]
        }
      ],
      "source": [
        "all_text = (test_text + train_text)\n",
        "single_test = list(set(test_text))\n",
        "single_train = list(set(train_text))\n",
        "single_all = list(set(all_text))\n",
        "\n",
        "print(len(single_all),len(single_test),len(single_train), len(single_train + single_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VmgQGgkiwbMf",
        "outputId": "a0e18980-331d-4dfa-82c4-c4092a2c0825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O\tchip\tO\n",
            "B-Gtype\t-\tGtype\n",
            "O\tf\tO\n",
            "O\t##nr\tO\n",
            "B-Supp\t_\tSupp\n",
            "O\tf\tO\n",
            "O\t##nr\tO\n",
            "O\t##8\tO\n",
            "O\t##my\tO\n",
            "O\t##c\tO\n",
            "B-Supp\t_\tSupp\n",
            "O\tglucose\tO\n",
            "B-Air\t_\tSupp\n",
            "O\tnh\tO\n",
            "O\t##4\tO\n",
            "O\t##cl\tO\n",
            "\n",
            "\n",
            "O\te\tO\n",
            "O\t.\tO\n",
            "O\tcoli\tO\n",
            "O\tk\tO\n",
            "O\t-\tO\n",
            "O\t12\tO\n",
            "O\tmg\tO\n",
            "O\t##16\tO\n",
            "O\t##55\tO\n",
            "O\tw\tO\n",
            "O\t##t\tO\n",
            "O\t,\tO\n",
            "O\tand\tO\n",
            "O\tδ\tO\n",
            "O\t##cr\tO\n",
            "O\t##a\tO\n",
            "B-Phase\twere\tO\n",
            "I-Phase\tgrown\tO\n",
            "B-Air\tto\tAir\n",
            "O\tmid\tO\n",
            "B-Temp\t-\tAir\n",
            "O\tlog\tO\n",
            "B-Med\tphase\tO\n",
            "I-Med\taero\tMed\n",
            "I-Med\t##bic\tO\n",
            "I-Med\t##ally\tMed\n",
            "I-Med\tat\tO\n",
            "O\t37\tO\n",
            "O\t##°\tO\n",
            "O\t##c\tO\n",
            "O\tin\tO\n",
            "B-Supp\tm\tO\n",
            "B-Supp\t##9\tO\n",
            "I-Supp\tminimal\tO\n",
            "O\tmedia\tSupp\n",
            "O\tsupplemented\tO\n",
            "\n",
            "\n",
            "B-Air\taero\tAir\n",
            "B-Air\t##bic\tAir\n",
            "O\tcultures\tO\n",
            "\n",
            "\n",
            "O\tmedium\tO\n",
            "O\t:\tO\n",
            "B-Med\tlb\tSupp\n",
            "\n",
            "\n",
            "O\tcells\tO\n",
            "O\twere\tO\n",
            "O\tgrown\tO\n",
            "O\taero\tO\n",
            "O\t##bic\tO\n",
            "O\t##ally\tO\n",
            "O\t(\tAir\n",
            "O\t70\tAir\n",
            "O\t%\tAir\n",
            "O\tn\tAir\n",
            "O\t##2\tAir\n",
            "O\t,\tAir\n",
            "O\t25\tAir\n",
            "O\t%\tAir\n",
            "O\to\tAir\n",
            "O\t##2\tAir\n",
            "O\t,\tAir\n",
            "O\tand\tAir\n",
            "O\t5\tAir\n",
            "O\t%\tAir\n",
            "O\tco\tAir\n",
            "O\t##2\tAir\n",
            "O\t)\tO\n",
            "O\tor\tO\n",
            "B-Air\tana\tO\n",
            "B-Air\t##ero\tO\n",
            "B-Air\t##bic\tO\n",
            "B-Air\t##ally\tO\n",
            "I-Air\t(\tAir\n",
            "I-Air\t95\tO\n",
            "I-Air\t%\tAir\n",
            "I-Air\tn\tAir\n",
            "I-Air\t##2\tAir\n",
            "I-Air\tand\tO\n",
            "I-Air\t5\tAir\n",
            "I-Air\t%\tAir\n",
            "I-Air\tco\tAir\n",
            "I-Air\t##2\tAir\n",
            "I-Air\t)\tO\n",
            "O\tuntil\tO\n",
            "B-Phase\tmid\tO\n",
            "I-Phase\t-\tO\n",
            "B-OD\tlog\tO\n",
            "I-OD\tphase\tO\n",
            "I-OD\to\tOD\n",
            "I-OD\t##d\tO\n",
            "I-OD\t##60\tO\n",
            "I-OD\t##0\tO\n",
            "O\tof\tO\n",
            "\n",
            "\n",
            "O\tstrain\tO\n",
            "O\t:\tO\n",
            "B-Strain\tk\tO\n",
            "0\t-\tGtype\n",
            "\n",
            "\n",
            "O\t32\tO\n",
            "B-Temp\t43\tO\n",
            "B-Temp\t##°\tO\n",
            "B-Temp\t##c\tTemp\n",
            "O\trep\tO\n",
            "O\t##2\tO\n",
            "\n",
            "\n",
            "O\tgen\tO\n",
            "O\t##otype\tO\n",
            "O\t/\tO\n",
            "O\tvariation\tO\n",
            "O\t:\tO\n",
            "\n",
            "\n",
            "B-Gtype\tδ\tGtype\n",
            "B-Gtype\t##ox\tGtype\n",
            "B-Gtype\t##yr\tGtype\n",
            "B-Supp\tp\tSupp\n",
            "B-Supp\t##q\tSupp\n",
            "O\t2\tO\n",
            "\n",
            "\n",
            "B-Gtype\t∆\tGtype\n",
            "B-Gtype\t##ry\tGtype\n",
            "B-Gtype\t##h\tGtype\n",
            "B-Gtype\t##b\tGtype\n",
            "B-Air\tana\tAir\n",
            "B-Air\t##ero\tAir\n",
            "B-Air\t##bic\tO\n",
            "\n",
            "\n",
            "O\tip\tO\n",
            "O\tantibody\tO\n",
            "O\t:\tO\n",
            "B-Anti\taffinity\tAnti\n",
            "I-Anti\tpu\tO\n",
            "I-Anti\t##rified\tO\n",
            "I-Anti\tanti\tAnti\n",
            "I-Anti\t-\tGtype\n",
            "\n",
            "\n",
            "O\tfur\tO\n",
            "O\tip\tO\n",
            "B-Technique\tchip\tTechnique\n",
            "B-Air\t-\tAir\n",
            "B-Supp\tse\tO\n",
            "B-Supp\t##q\tO\n",
            "O\tana\tAir\n",
            "O\t##ero\tO\n",
            "O\t##bic\tAir\n",
            "O\tiron\tSupp\n",
            "\n",
            "\n",
            "B-Gtype\tw\tGtype\n",
            "B-Gtype\t##t\tGtype\n",
            "B-Supp\tace\tSupp\n",
            "B-Supp\t##tate\tSupp\n",
            "\n",
            "\n",
            "B-Gtype\tδ\tGtype\n",
            "B-Gtype\t##fur\tGtype\n",
            "O\twith\tSupp\n",
            "B-Supp\tfe\tSupp\n",
            "O\t2\tO\n",
            "B-Technique\t(\tTechnique\n",
            "I-Technique\trna\tTechnique\n",
            "I-Technique\t-\tTechnique\n",
            "I-Technique\tse\tTechnique\n",
            "I-Technique\t##q\tTechnique\n",
            "\n",
            "\n",
            "B-Gtype\tδ\tGtype\n",
            "B-Gtype\t##ga\tGtype\n",
            "B-Gtype\t##d\tGtype\n",
            "B-Gtype\t##x\tGtype\n",
            "B-pH\tph\tpH\n",
            "B-pH\t##5\tpH\n",
            "O\t.\tO\n",
            "\n",
            "\n",
            "O\tes\tO\n",
            "O\t##cher\tO\n",
            "O\t##ichi\tO\n",
            "O\t##a\tO\n",
            "O\tcoli\tO\n",
            "O\tmg\tO\n",
            "O\t##16\tO\n",
            "O\t##55\tO\n",
            "O\tk\tO\n",
            "O\t-\tO\n",
            "O\t12\tO\n",
            "O\tw\tO\n",
            "O\t##t\tO\n",
            "O\tand\tO\n",
            "O\t∆\tO\n",
            "O\t##f\tO\n",
            "O\t##nr\tO\n",
            "O\twere\tO\n",
            "B-Phase\tgrown\tO\n",
            "I-Phase\tto\tO\n",
            "B-OD\tmid\tO\n",
            "I-OD\t-\tO\n",
            "B-Air\tlog\tO\n",
            "I-Air\tphase\tO\n",
            "I-Air\to\tO\n",
            "I-Air\t.\tO\n",
            "I-Air\td\tO\n",
            "I-Air\t.\tO\n",
            "I-Air\t600\tO\n",
            "I-Air\t##n\tO\n",
            "I-Air\t##m\tO\n",
            "I-Air\t0\tO\n",
            "I-Air\t.\tO\n",
            "I-Air\t3\tO\n",
            "O\tan\tO\n",
            "O\t##ero\tO\n",
            "O\t##bic\tO\n",
            "O\t##ally\tO\n",
            "B-Temp\t(\tSupp\n",
            "O\t95\tO\n",
            "B-Med\t%\tAir\n",
            "B-Supp\tn\tAir\n",
            "B-Supp\t##2\tAir\n",
            "I-Supp\t,\tSupp\n",
            "I-Supp\t5\tO\n",
            "O\t%\tAir\n",
            "O\tco\tO\n",
            "O\t##2\tAir\n",
            "O\t)\tO\n",
            "O\tat\tO\n",
            "O\t37\tTemp\n",
            "\n",
            "\n",
            "O\tgen\tO\n",
            "O\t##otype\tO\n",
            "O\t:\tO\n",
            "\n",
            "\n",
            "O\tat\tO\n",
            "B-OD\to\tO\n",
            "B-OD\t##d\tO\n",
            "B-OD\t##45\tO\n",
            "B-OD\t##0\tO\n",
            "I-OD\t=\tO\n",
            "I-OD\t0\tO\n",
            "O\t.\tO\n",
            "O\t3\tO\n",
            "O\t,\tO\n",
            "O\tcultures\tO\n",
            "B-Supp\tinduced\tO\n",
            "I-Supp\twith\tO\n",
            "I-Supp\t1\tSupp\n",
            "0\tmm\tSupp\n",
            "0\tip\tSupp\n",
            "0\t##t\tSupp\n",
            "0\t##g\tSupp\n",
            "\n",
            "\n",
            "O\tgen\tO\n",
            "O\t##otype\tO\n",
            "O\t/\tO\n",
            "O\tvariation\tO\n",
            "O\t:\tO\n",
            "O\tharbor\tO\n",
            "O\t##ing\tO\n",
            "B-Gtype\tf\tGtype\n",
            "B-Gtype\t##nr\tGtype\n",
            "0\t-\tGtype\n",
            "\n",
            "\n",
            "O\tσ\tGtype\n",
            "O\t##32\tGtype\n",
            "B-Temp\t30\tO\n",
            "B-Temp\t##°\tO\n",
            "B-Temp\t##c\tO\n",
            "O\trep\tO\n",
            "O\t##1\tO\n",
            "\n",
            "\n",
            "O\tgen\tO\n",
            "O\t##oy\tO\n",
            "O\t##pe\tO\n",
            "O\t:\tO\n",
            "B-Gtype\td\tSupp\n",
            "B-Gtype\t##f\tSupp\n",
            "B-Gtype\t##nr\tSupp\n",
            "\n",
            "\n",
            "O\tfur\tO\n",
            "O\tip\tO\n",
            "B-Technique\tchip\tTechnique\n",
            "B-Air\t-\tAir\n",
            "O\tse\tO\n",
            "O\t##q\tO\n",
            "\n",
            "\n",
            "O\tculture\tO\n",
            "O\tcondition\tO\n",
            "O\t:\tO\n",
            "B-Air\tana\tAir\n",
            "B-Air\t##ero\tAir\n",
            "B-Air\t##bic\tAir\n",
            "O\tcultures\tO\n",
            "\n",
            "\n",
            "O\tchip\tO\n",
            "O\tantibody\tO\n",
            "O\t:\tO\n",
            "B-Anti\t32\tO\n",
            "\n",
            "\n",
            "O\tchip\tO\n",
            "B-Gtype\t-\tGtype\n",
            "O\tf\tO\n",
            "O\t##nr\tO\n",
            "B-Supp\t_\tSupp\n",
            "O\tf\tO\n",
            "O\t##nr\tO\n",
            "O\t##8\tO\n",
            "O\t##my\tO\n",
            "O\t##c\tO\n",
            "B-Supp\t_\tSupp\n",
            "O\tglucose\tO\n",
            "B-Supp\t_\tSupp\n",
            "O\tnh\tO\n",
            "O\t##4\tO\n",
            "O\t##cl\tO\n",
            "\n",
            "\n",
            "O\tgen\tO\n",
            "O\t##otype\tO\n",
            "O\t:\tO\n",
            "B-Gtype\tw\tGtype\n",
            "B-Gtype\t##t\tGtype\n",
            "\n",
            "\n",
            "B-Gtype\tw\tGtype\n",
            "B-Gtype\t##t\tGtype\n",
            "B-Supp\tglucose\tSupp\n",
            "O\t1\tO\n",
            "\n",
            "\n",
            "O\tgen\tO\n",
            "O\t##oy\tO\n",
            "O\t##pe\tO\n",
            "O\t:\tO\n",
            "B-Gtype\twild\tGtype\n",
            "\n",
            "\n",
            "B-Substrain\tmg\tGtype\n",
            "B-Substrain\t##16\tGtype\n",
            "B-Substrain\t##55\tO\n",
            "\n",
            "\n",
            "O\taf\tO\n",
            "O\t##fy\tO\n",
            "O\t##ex\tO\n",
            "O\t##p\tO\n",
            "B-Gtype\t_\tGtype\n",
            "O\tdelta\tO\n",
            "B-Supp\t-\tSupp\n",
            "O\tarc\tO\n",
            "O\t##a\tO\n",
            "B-Supp\t_\tSupp\n",
            "O\tglucose\tO\n",
            "B-Air\t_\tSupp\n",
            "O\tnh\tO\n",
            "O\t##4\tO\n",
            "O\t##cl\tO\n",
            "\n",
            "\n",
            "O\tcsi\tO\n",
            "O\t##r\tO\n",
            "B-Technique\t_\tTechnique\n",
            "\n",
            "\n",
            "B-Gtype\tw\tGtype\n",
            "B-Gtype\t##t\tGtype\n",
            "B-Supp\tna\tSupp\n",
            "B-Supp\t##cl\tSupp\n",
            "\n",
            "\n",
            "O\tpt\tO\n",
            "O\t##7\tO\n",
            "O\t_\tGtype\n",
            "B-Technique\tchips\tTechnique\n",
            "B-Technique\t##e\tTechnique\n",
            "B-Technique\t##q\tTechnique\n",
            "\n",
            "\n",
            "O\t32\tO\n",
            "B-Temp\t30\tO\n",
            "B-Temp\t##°\tO\n",
            "B-Temp\t##c\tO\n",
            "O\trep\tO\n",
            "O\t##2\tO\n",
            "\n",
            "\n",
            "B-Gtype\tδ\tGtype\n",
            "B-Gtype\t##cr\tGtype\n",
            "B-Gtype\t##a\tGtype\n",
            "B-Supp\tfr\tSupp\n",
            "B-Supp\t##uc\tSupp\n",
            "B-Supp\t##tose\tSupp\n",
            "O\t2\tO\n",
            "\n",
            "\n",
            "O\tar\tO\n",
            "O\t##gr\tO\n",
            "B-Supp\t_\tSupp\n",
            "O\tar\tO\n",
            "O\t##gin\tO\n",
            "O\t##ine\tO\n",
            "\n",
            "\n",
            "O\tstrain\tO\n",
            "O\t:\tO\n",
            "\n",
            "\n",
            "O\tchip\tO\n",
            "B-Gtype\t-\tGtype\n",
            "O\tarc\tO\n",
            "O\t##a\tO\n",
            "B-Supp\t_\tSupp\n",
            "O\tarc\tO\n",
            "O\t##a\tO\n",
            "O\t##8\tO\n",
            "O\t##my\tO\n",
            "O\t##c\tO\n",
            "B-Supp\t_\tSupp\n",
            "O\tglucose\tO\n",
            "B-Supp\t_\tSupp\n",
            "O\tnh\tO\n",
            "O\t##4\tO\n",
            "O\t##cl\tO\n",
            "\n",
            "\n",
            "O\tsox\tO\n",
            "O\t##s\tGtype\n",
            "B-Supp\tp\tSupp\n",
            "B-Supp\t##q\tSupp\n",
            "O\t2\tO\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for sen_idx, pred_sentence in enumerate(sentence_tags):\n",
        "    for word_idx, pred_tok in enumerate(pred_sentence):\n",
        "        print(exp_labels[sen_idx][word_idx+1],end='\\t')\n",
        "        print((pre_txt[sen_idx][word_idx]),end='\\t')\n",
        "        print(sentence_tags[sen_idx][word_idx])\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHM7Qt3wwbMf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copia de MentionModel.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "51256fb29d3417d51258ec6b95047b6f4277a1591b5c1cc3248ea847bb42ce8c"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('DeepLearning')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
